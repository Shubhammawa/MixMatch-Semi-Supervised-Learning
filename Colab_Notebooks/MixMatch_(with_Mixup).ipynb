{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MixMatch_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1J2e405NyMr4tdLrM1L-bRRpYNkiuRL-L",
      "authorship_tag": "ABX9TyM5E6fGiFxUqjGwclH7W3/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b324ea92760d40bd8425d415042b00a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c549f7d4d87425e97ea9b93fa31f8d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd9e8f1e045d4de8850bfcfecaa49058",
              "IPY_MODEL_7418179f4df2462c993d23440cb1a0e3"
            ]
          }
        },
        "0c549f7d4d87425e97ea9b93fa31f8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd9e8f1e045d4de8850bfcfecaa49058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c9aef14437824258915dfb75cdfd358e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 138223492,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 138223492,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5539a32b402c4aec97fb794bf059a41f"
          }
        },
        "7418179f4df2462c993d23440cb1a0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5d03f0048d9f490c81dd4bd71660870f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 132M/132M [1:15:32&lt;00:00, 30.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02f6263d0068411f8e8298866a1dd73a"
          }
        },
        "c9aef14437824258915dfb75cdfd358e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5539a32b402c4aec97fb794bf059a41f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d03f0048d9f490c81dd4bd71660870f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02f6263d0068411f8e8298866a1dd73a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awl-shubham-mawa/AWL-Internship/blob/master/Copy_of_MixMatch_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDZ42kpnAy0y",
        "colab_type": "text"
      },
      "source": [
        "### 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euD3qvtNQGxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch.utils.data import random_split\n",
        "from torchsummary import summary\n",
        "from torchsummary import summary\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWC4cIHWA28-",
        "colab_type": "text"
      },
      "source": [
        "### 2. Filepaths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywmc3z9BU3cQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AAF_TRAIN_PATH = 'drive/My Drive/AWL Internship/MixMatch/AAF_Gender_Classification/AAF_train_MixMatch.xlsx'\n",
        "AAF_TEST_PATH = 'drive/My Drive/AWL Internship/MixMatch/AAF_Gender_Classification/AAF_test_MixMatch.xlsx'\n",
        "!unzip -q 'drive/My Drive/AWL Internship/AAF Dataset/extracted_original-20'\n",
        "AAF_IMAGE_PATH = 'extracted_original-20'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFaTBNxEA8ai",
        "colab_type": "text"
      },
      "source": [
        "### 3. Arguments/Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYB0WVHUU9XW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "22a302a6-1c57-4de3-8b83-25ad15fe3ed3"
      },
      "source": [
        "CUDA = 0\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "lambda_u = 1\n",
        "NUM_LABELLED = 500\t#No of labelled examples to be used in MixMatch\n",
        "DEVICE = torch.device(\"cuda:%d\" % CUDA)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r6WmimzBA6L",
        "colab_type": "text"
      },
      "source": [
        "### 4. AAF Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex-ZykbPVBkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AAF_Dataset(Dataset):\n",
        "\t''' Custom Dataset for loading AAF Dataset images'''\n",
        "\n",
        "\tdef __init__(self, csv_path, img_dir, transform):\n",
        "\t\t\n",
        "\t\tdf = pd.read_excel(csv_path)\n",
        "\t\tself.img_dir = img_dir\n",
        "\t\tself.transform = transform\n",
        "\t\tself.csv_path = csv_path\n",
        "\t\tself.gender = df['Gender'].values\n",
        "\t\tself.filename = df['Image'].values\n",
        "\t\n",
        "\t#def preprocess(self):\n",
        "\t''' Any further preprocessing required in the data\n",
        "\t\tcan be performed here'''\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\n",
        "\t\timg = Image.open(os.path.join(self.img_dir,\n",
        "\t\t\t\t\t\t\t\t\tself.filename[index]))\n",
        "\t\timg = self.transform(img)\n",
        "\t\ty_true = self.gender[index]\n",
        "\t\ty_true = torch.tensor(y_true, dtype=torch.float32)\n",
        "\t\t\n",
        "\t\treturn img, y_true\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.gender.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSfgCtpABI5-",
        "colab_type": "text"
      },
      "source": [
        "### 5. Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a76U-gWoBHA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_transform = transforms.Compose([transforms.Resize((96,96)),\n",
        "\t\t\t\t\t\t\ttransforms.ToTensor()])\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIMqD2c9BN-o",
        "colab_type": "text"
      },
      "source": [
        "### 6. Sample batch for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOuSyI_0eHe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "94cb612c-05b1-493d-e209-1f835d197609"
      },
      "source": [
        "sample_batch_size = 4\n",
        "sample_dataset = AAF_Dataset(csv_path=AAF_TRAIN_PATH, img_dir=AAF_IMAGE_PATH, transform=custom_transform)\n",
        "sample_loader = DataLoader(dataset=sample_dataset, batch_size=sample_batch_size, shuffle=True)\n",
        "\n",
        "dataiter = iter(sample_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print(\"Batch shape (images): \",images.shape)\n",
        "print(\"Batch shape (labels): \", labels.shape)\n",
        "#print(y_true.shape)\n",
        "\n",
        "#print(images[0])\n",
        "# print(images[0].shape)\n",
        "# print(labels[0].item())\n",
        "#print(y_true[0])\n",
        "\n",
        "def imshow(img, title):\n",
        "    '''Function imshow: Helper function to display an image'''\n",
        "    plt.figure(figsize=(sample_batch_size * 4, 4))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def show_batch_images(dataloader):\n",
        "    '''Function show_batch_images: Helper function to display images with their true ages'''\n",
        "    images, labels = next(iter(dataloader))\n",
        "    \n",
        "    img = torchvision.utils.make_grid(images)\n",
        "    imshow(img, title = 'Images')\n",
        "    print(\"Labels: \",labels)\n",
        "    \n",
        "    return images, labels\n",
        "images, labels = show_batch_images(sample_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeKdspecBjOt",
        "colab_type": "text"
      },
      "source": [
        "### 7. Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0DGgV_eeOaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "fe9c0e43-2cf5-46cb-b236-587e39685bfe"
      },
      "source": [
        "AAF_train = AAF_Dataset(csv_path=AAF_TRAIN_PATH, img_dir=AAF_IMAGE_PATH, transform=custom_transform)\n",
        "\n",
        "AAF_train_labelled, AAF_train_unlabelled = random_split(AAF_train, [NUM_LABELLED, len(AAF_train) - NUM_LABELLED])\n",
        "\n",
        "trainloader_labelled = DataLoader(AAF_train_labelled, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainloader_unlabelled = DataLoader(AAF_train_unlabelled, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "AAF_test = AAF_Dataset(csv_path=AAF_TEST_PATH, img_dir=AAF_IMAGE_PATH, transform=custom_transform)\n",
        "\n",
        "testloader = DataLoader(AAF_test, batch_size= BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Labelled examples: \" + str(len(AAF_train_labelled)) + \"\\nUnlabelled examples: \" \n",
        "      + str(len(AAF_train_unlabelled)) + \"\\nTest examples: \" + str(len(AAF_test)))\n",
        "\n",
        "# dataiter = iter(trainloader_labelled)\n",
        "# images, labels = dataiter.next()\n",
        "# print(labels[:])\n",
        "# print(len(AAF_train_labelled), len(AAF_train_unlabelled))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9PYMisSBqQN",
        "colab_type": "text"
      },
      "source": [
        "### 8. MixMatch Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q5DyxYPeSjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment_image(batch_img, K = 2):\n",
        "\t'''Function augment_image:\n",
        "\t\tInput: PIL Image/Torch Tensor\n",
        "\t\tOutput: K number of augmented images'''\n",
        "\t\n",
        "\tbatch_augment_images = []\n",
        "\tfor i in range(batch_img.shape[0]):\n",
        "\t\timg = batch_img[i]\n",
        "\t\timg = TF.to_pil_image(img.cpu())\n",
        "\t\timg_1 = TF.to_tensor(TF.adjust_brightness(img, np.random.uniform(0.5, 1.5)))\n",
        "\t\timg_2 = TF.to_tensor(TF.adjust_contrast(img, np.random.uniform(0.5, 1.5)))\n",
        "\t\timg_3 = TF.to_tensor(TF.adjust_saturation(img, np.random.uniform(0.5, 1.5)))\n",
        "\t\t\n",
        "\t\timg_4 = TF.to_tensor(TF.hflip(img))\n",
        "\t\timg_5 = TF.to_tensor(TF.rotate(img, angle=np.random.uniform(-10,10)))\n",
        "\n",
        "\t\timg_6 = TF.to_tensor(TF.to_grayscale(img, num_output_channels=3))\n",
        "\t\timg_7 = TF.to_tensor(TF.adjust_gamma(img, np.random.uniform(0.5, 1.5)))\n",
        "\n",
        "\t\trandom_numbers = random.sample(range(1, 8), K)\n",
        "\t\timg_dict = {'1': img_1, '2': img_2, '3': img_3, '4': img_4, '5': img_5, '6': img_6, '7': img_7}\n",
        "\n",
        "\t\taugment_images = []\n",
        "\t\tfor i in random_numbers:\n",
        "\t\t\taugment_images.append(img_dict[str(i)])\n",
        "\t\tbatch_augment_images.append(augment_images)\n",
        "\treturn batch_augment_images\n",
        "\n",
        "def label_guessing(model, augment_images, device):\n",
        "\t''' Function label_guessing\n",
        "\t\tInput: Classifier model, K augmentations of the unlabelled data\n",
        "\t\tOuput: Calls augment_image function, makes predictions for the K augmentations and averages them to get the guessed\n",
        "\t\t\t\tlabels for unlabelled data.\n",
        "\t\t'''\n",
        "\tpredictions = []\n",
        "\n",
        "\tfor i in range(0,len(augment_images)):\n",
        "\n",
        "\t\timg = torch.stack(augment_images[i], dim=0)\n",
        "\t\timg = img.to(device)\n",
        "\t\tlogits = model(img)\n",
        "\t\tprobas = nn.functional.softmax(logits, dim=1)\n",
        "\t\tpredictions.append(probas)\n",
        "\tpredictions = torch.stack(predictions,dim=0)\n",
        "\tq_hat = torch.mean(predictions, dim=1)\n",
        "\n",
        "\treturn q_hat\n",
        "\n",
        "def sharpen(p, T=0.5):\n",
        "\tp_sharp = torch.pow(p, 1/T)/(torch.sum(torch.pow(p, 1/T), dim=0))\n",
        "\treturn p_sharp\n",
        "\n",
        "def mixup(x1,y1,x2,y2,alpha=0.75):\n",
        "    l = np.random.beta(alpha,alpha)\n",
        "    l = max(l,1-l)\n",
        "    x = l * x1 + (1-l) * x2\n",
        "    y = l* y1 + (1-l) * y2\n",
        "    return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa-wmJhUBu_K",
        "colab_type": "text"
      },
      "source": [
        "### 9. MixMatch Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lkQ9e9DeYv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MixMatch_Dataset(Dataset):\n",
        "\t'''Supply a batch of labelled and unlabelled data, X and U.'''\n",
        "\t\n",
        "\tdef __init__(self, Labelled_data, Unlabelled_data):\n",
        "\t\tself.Labelled_data = Labelled_data\n",
        "\t\tself.Unlabelled_data = Unlabelled_data\n",
        "  \n",
        "\tdef __getitem__(self, index):\n",
        "\t\t\n",
        "\t\tsize_labelled = len(self.Labelled_data)\n",
        "\t\tsize_unlabelled = len(self.Unlabelled_data)\n",
        "\t\t\n",
        "\t\tif(index < size_labelled):\n",
        "\t\t\tl_index = index\n",
        "\t\t\t\n",
        "\t\telse:\n",
        "\t\t\tl_index = int(index*len(self.Labelled_data)/len(self.Unlabelled_data))\n",
        "\n",
        "\t\tif(index < size_unlabelled):\n",
        "\t\t\tu_index = index\n",
        "\t\telse:\n",
        "\t\t\tu_index = index - size_unlabelled\n",
        "\n",
        "\t\tx = self.Labelled_data[l_index][0]\n",
        "\t\ty = self.Labelled_data[l_index][1]\n",
        "\t\tu = self.Unlabelled_data[u_index][0]\n",
        "\n",
        "\t\treturn x, y, u\n",
        "\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn max(len(self.Labelled_data), len(self.Unlabelled_data))\n",
        "\n",
        "MixMatch_dataset = MixMatch_Dataset(Labelled_data=AAF_train_labelled, Unlabelled_data=AAF_train_unlabelled)\n",
        "MixMatch_loader = DataLoader(MixMatch_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkhnU7ZvCELo",
        "colab_type": "text"
      },
      "source": [
        "### 10. Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISLQQAVevNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = nn.CrossEntropyLoss(reduction='sum')\n",
        "l2_loss = nn.MSELoss(reduction='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et2WFMTgCKVm",
        "colab_type": "text"
      },
      "source": [
        "### 11. Wide-Resnet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOCKWPqcezTT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b324ea92760d40bd8425d415042b00a0",
            "0c549f7d4d87425e97ea9b93fa31f8d7",
            "fd9e8f1e045d4de8850bfcfecaa49058",
            "7418179f4df2462c993d23440cb1a0e3",
            "c9aef14437824258915dfb75cdfd358e",
            "5539a32b402c4aec97fb794bf059a41f",
            "5d03f0048d9f490c81dd4bd71660870f",
            "02f6263d0068411f8e8298866a1dd73a"
          ]
        },
        "outputId": "e4cd4245-4a5a-42bb-84ff-560a964d8e4d"
      },
      "source": [
        "model_1 = models.wide_resnet50_2(pretrained=True)\n",
        "model_1.to(DEVICE)\n",
        "print(summary(model_1, (3, 96, 96)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-kR8-f3Cldq",
        "colab_type": "text"
      },
      "source": [
        "### 12. Gender Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mNC-1Lye1cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Gender_Classifier(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Gender_Classifier, self).__init__()\n",
        "\n",
        "            self.fc1 = nn.Linear(1000, 100)\n",
        "            self.fc2 = nn.Linear(100, 10)\n",
        "            self.fc3 = nn.Linear(10,2)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.fc1(x)\n",
        "            x = self.fc2(x)\n",
        "            logits = self.fc3(x)\n",
        "            return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJywJaOACv9_",
        "colab_type": "text"
      },
      "source": [
        "### 13. Stack classifier onto the Wide-Resnet architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEFMAIfGe4HK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6eb0fc6-522d-43b5-eda0-829e7561794d"
      },
      "source": [
        "model_2 = Gender_Classifier()\n",
        "model = nn.Sequential(model_1, model_2)\n",
        "model.to(DEVICE)\n",
        "print(summary(model, (3,96,96)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB9oJRZVC05C",
        "colab_type": "text"
      },
      "source": [
        "### 14. Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh3MmOxCe6_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NcBNcNkC4ht",
        "colab_type": "text"
      },
      "source": [
        "### 15. Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEWS5C1Ke8-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "e77ec1cd-6efa-4557-b209-d5a12b186676"
      },
      "source": [
        "start_time = time.time()\n",
        "num_batches = 0\n",
        "#costs = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "\tmodel.train()\n",
        "\tfor batch_idx, (x, y, u) in enumerate(MixMatch_loader):\n",
        "\t\tx = x.to(DEVICE)\n",
        "\t\ty = y.to(DEVICE)\n",
        "\t\tu = u.to(DEVICE)\n",
        "\t\tnum_batches += 1\n",
        "\n",
        "\t\taugment_images = augment_image(u, K=4)\n",
        "\t\tq_hat = label_guessing(model, augment_images, device=DEVICE)\n",
        "\t\tq = torch.argmax(sharpen(q_hat), dim = 1)\n",
        "\t\t\n",
        "\t\twx = torch.cat([x,u])\n",
        "\t\twy = torch.cat([y,q])\n",
        "\n",
        "\t\tidx = torch.randperm(wx.shape[0])\n",
        "\n",
        "\t\tx_mix, y_mix = mixup(x, y, wx[idx[:int(len(idx)/2)]], wy[idx[:int(len(idx)/2)]])\n",
        "\t\tu_mix, q_mix = mixup(u, q, wx[idx[int(len(idx)/2):]], wy[idx[int(len(idx)/2):]])\n",
        "\n",
        "\t\ty_mix_pred = model(x_mix)\n",
        "\t\tq_mix_pred_logits = model(u_mix)\n",
        "\t\tq_mix_pred_probas = nn.functional.softmax(q_mix_pred_logits, dim=1)\n",
        "\t\tq_mix_pred = torch.argmax(q_mix_pred_probas, dim=1)\n",
        "\t\t\n",
        "\t\tcost_labelled = cross_entropy(y_mix_pred, y_mix.long())\n",
        "\t\tcost_unlabelled = l2_loss(q_mix_pred, q_mix)\n",
        "\n",
        "\t\tif(num_batches < 1000):\n",
        "\t\t\tramp = num_batches/1000\n",
        "\t\telse:\n",
        "\t\t\tramp = 1\n",
        "\t\tloss = cost_labelled + ramp*lambda_u*cost_unlabelled\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\t#costs.append(loss)\n",
        "\t\toptimizer.step()\n",
        "\t\tif(batch_idx % 25 == 0):\n",
        "\t\t\t# writer.add_scalar('Total Loss', loss, epoch*len(MixMatch_dataset) + batch_idx)\n",
        "\t\t\t# writer.add_scalar('Labelled Loss', cost_labelled, epoch*len(MixMatch_dataset) + batch_idx)\n",
        "\t\t\t# writer.add_scalar('Unlabelled Loss', cost_unlabelled, epoch*len(MixMatch_dataset) + batch_idx)\n",
        "\t\t\t# writer.add_scalars('Labelled and Unlabelled Loss', {'cost_labelled': cost_labelled,\n",
        "\t\t\t# \t\t\t\t\t\t\t\t\t\t\t\t\t'cost_unlabelled': cost_unlabelled}, \n",
        "\t\t\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t  epoch*len(MixMatch_dataset) + batch_idx)\n",
        "\t\t\t\n",
        "\t\t\ts = ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f | Labelled loss: %.4f | Unlabelled loss: %.4f\\n'\n",
        "\t\t\t\t% (epoch, NUM_EPOCHS, batch_idx,\n",
        "\t\t\t\t\tlen(MixMatch_dataset)//BATCH_SIZE, loss, cost_labelled, cost_unlabelled))\n",
        "\t\t\twith open('drive/My Drive/AWL Internship/MixMatch/Training Logs/train_log_'+str(NUM_LABELLED) + '.txt', 'a') as writefile:\n",
        "\t\t\t\twritefile.write(s)\n",
        "\t\n",
        "\ts = 'Time elapsed: %.2f min\\n' % ((time.time() - start_time)/60)\n",
        "\twith open('drive/My Drive/AWL Internship/MixMatch/Training Logs/train_log_'+str(NUM_LABELLED) + '.txt', 'a') as writefile:\n",
        "\t\twritefile.write(s)\n",
        "\tmodel.eval()\n",
        "\ty_true = []\n",
        "\ty_pred = []\n",
        "\twith torch.set_grad_enabled(False):\n",
        "\t\tfor batch_idx, (img, label) in enumerate(testloader):\n",
        "\t\t\timg = img.to(DEVICE)\n",
        "\t\t\tlabel = label.to(DEVICE)\n",
        "\n",
        "\t\t\tlogits = model(img)\n",
        "\t\t\tprobas = nn.functional.softmax(logits, dim=1)\n",
        "\t\t\tpred = torch.argmax(probas, dim=1)\n",
        "\t\t\ty_true.extend(label.cpu().numpy())\n",
        "\t\t\ty_pred.extend(pred.cpu().numpy())\n",
        "\tacc = accuracy_score(y_true, y_pred)\n",
        "\tf1 = f1_score(y_true, y_pred)\n",
        "\t# if(batch_idx % 50 == 0):\n",
        "\t\t# writer.add_scalar('F1-score', f1, epoch)\n",
        "\t\t# writer.add_scalar('Accuracy', acc, epoch)\n",
        "\twith open('drive/My Drive/AWL Internship/MixMatch/Training Logs/train_log_'+str(NUM_LABELLED) + '.txt', 'a') as writefile:\n",
        "\t\twritefile.write(\"Accuracy: \" + str(acc) + \"\\n\")\n",
        "\t\twritefile.write(\"F1_score: \" + str(f1) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdgVwvoMfAG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "41f94d88-8471-4ad8-9242-aad6c2107d85"
      },
      "source": [
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.set_grad_enabled(False):\n",
        "    correct_results = 0\n",
        "    for batch_idx, (img, label) in enumerate(testloader):\n",
        "        img = img.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "\n",
        "        logits = model(img)\n",
        "        probas = nn.functional.softmax(logits, dim=1)\n",
        "        pred = torch.argmax(probas, dim=1)\n",
        "        y_true.extend(label.cpu().numpy())\n",
        "        y_pred.extend(pred.cpu().numpy())\n",
        "print(accuracy_score(y_true, y_pred))\n",
        "print(f1_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGpXHuKvmRho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}